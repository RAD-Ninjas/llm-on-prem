# LLM-On-Prem

---

## Introduction

The frontend is a boilerplate app generated by ```npx create-secure-chatgpt-app```. It leverages several of Pangea's security services for securing ChatGPT usage.

## Features

- **Frontend:**
  - Authentication using Pangea AuthN service and NextJS framework
  - Secure `chat` page
  - Secure `chat/generate` api endpoint
  - Redacting user prompts using Pangea's Redact service
  - Auditing user prompts using Pangea's Secure Audit Log service
  - De-fanging malicious responses from OpenAI API using Pangea's Domain Intel and URL Intel services

- **Backend:**
  - lm-sys [FastChat server](https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md) configured to operate as a stand-in for the OpenAI API
  - VLLM inference engine worker
  - Supports multiple HF transformer models

## Installation

Once you've cloned the repo, you can can build the docker images and run the containers using the following commands:

- Install docker-compose and build the images:

  ```
  pip install -y docker-compose
  ```

- Build and Run the containers:
  
  - **CPU:**
    - Build `docker-compose --profile cpu build`
    - Run `docker-compose --profile cpu up`
  <br/>
  - **CUDA:**
    - Build `docker-compose --profile cuda build`
    - Run `docker-compose --profile cuda up`
  <br/>
  - **OpenVINO:**
    - Build `docker-compose --profile openvino build`
    - Run `docker-compose --profile openvino up`
  <br/>
  - **Metal:**
    - Build `docker-compose --profile metal build`
    - Run `docker-compose --profile metal up`
  <br/>
  - **GGML:**
    - Build `docker-compose --profile ggml build`
    - Run `docker-compose --profile ggml up`
